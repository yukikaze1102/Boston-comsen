上次采用的是机器学习的方法，这次使用pytorch用深度学习来解决这个问题
——————————————
基于上次会议我学到了很多东西：
1.数据处理：我了解了数据怎么样更好的处理（除了标准化消除量纲影响以外），还有独热算法来处理一些异常值包括缺失值，还有主动增加变量来更好的对问题进行处理
2.神经网络处理：我这次引入了激活函数Relu来处理非线性问题，同时调整了隐藏层数和维度使之拟合的更好，下次可能会研究一些正则化惩罚以及dropout的应用
3.算法优化：我了解了adam优化器和随机梯度下降等一些优化方法，通过使用这些优化方法我确实让结果好了不少
4.ONNX模型：第一次知道了Netron的存在，而且这个可视化神经网络确实让我对整个模式有了更深的理解，以前对神经网络的感觉就是很抽象（怎么通过这些参数就得到结果了？？？），现在明白了输入端和输出端的关系，以及了解了几个激活函数
5.其他：包括k折验证，早停法一些方法，总之我对深度学习有了更深的认识。因为我此前一直看的是机器学习方面的知识，并且对这二者的理解存在一些误区，这次实践很好的帮助我跨过了这道门槛，也让以后的学习更得心应手了

这两周开学有点忙碌并没有怎么学习实验室的内容，等这周末我会再去了解“分类"相关的问题,以上

下面是一些核心的代码阐释

一：3sigma异常值处理
σ原则，又叫拉依达原则，是一基于正态分布的数学原理，它假设一组检测数据中只含有随机误差，通过计算得到标准偏差σ，
然后按一定概率确定一个区间，对于超过这个区间的误差，就不属于随机误差而是粗大误差，将含有粗大误差的数据进行剔除.
在统计学中，如果一个变量服从正态分布，且它的均值是u, 标准差是σ，那么将有：
（1）68.27%的数据会落在 u ± σ 内，即数据分布在处于(u−σ, u+σ)中的概率是0.6827
（2）95.45%的数据会落在 u ± 2σ 内，即数据分布在处于(u−2σ, u+2σ)中的概率是0.9545
（3）99.73%的数据会落在 u ± 3σ 内，即数据分布在处于(u−3σ, u+3σ)中的概率是0.9973

def handle_outliers(data, n=3):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    upper = mean + n* std
    lower = mean - n* std
    return np.clip(data, lower, upper)
X = handle_outliers(X)
y = handle_outliers(y)

二：神经网络构建

class BostonModel(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1)
        )
        
输入层 (nn.Linear(input_dim, 128)) 
将特征维度从input_dim映射到128维，增加模型的表达能力。
ReLU激活函数
引入非线性，使模型能够学习复杂模式。相比Sigmoid，ReLU计算更快且缓解梯度消失
Dropout(0.3)
训练时随机屏蔽30%的神经元，减少对特定特征的依赖，防止过拟合
隐藏层 (nn.Linear(128, 64))
压缩特征至64维，逐步提取高阶特征
输出层 (nn.Linear(64, 1))
输出单个连续值（房价），无激活函数，适用于回归任务


  ——————这是早停法      
class EarlyStopper:
    def __init__(self, patience=10, min_delta=0.01):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_loss = float('inf')
        self.best_weights = None

    def check_early_stop(self, model, current_loss):
        if current_loss < self.min_loss - self.min_delta:
            self.min_loss = current_loss
            self.counter = 0
            # 保存最佳权重
            self.best_weights = model.state_dict().copy()
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                # 恢复最佳权重
                if self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                return True
        return False



超参数
batch_size = 32
num_epochs = 300
patience = 15


batch_size=32	共约 506 个样本，32 的批次大小约需 16 步/epoch，避免小批量噪声过大或大批量内存不足
num_epochs=300	即遍历次数，提供充足训练时间，配合早停灵活终止，避免欠拟合或过拟合
patience=15	监控模型收敛趋势，就是能容忍早停的次数


    初始化模型、损失函数和优化器
    model = BostonModel(input_dim=X_train.shape[1])
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

   
# ------------------------------
#  可视化优化（统一绘图样式）
# ------------------------------
可视化用了deepseek，相应绘图代码还没学

def plot_loss_curves(train_losses, val_losses, title):
    plt.figure(figsize=(10, 6))
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

    max_epochs = max(len(loss) for loss in train_losses)
    x_axis = np.arange(1, max_epochs + 1)

    for i, (t_loss, v_loss) in enumerate(zip(train_losses, val_losses)):
        # 对齐不同长度的损失序列
        extended_t_loss = np.pad(t_loss, (0, max_epochs - len(t_loss)),
                                 mode='constant', constant_values=np.nan)
        extended_v_loss = np.pad(v_loss, (0, max_epochs - len(v_loss)),
                                 mode='constant', constant_values=np.nan)

        plt.plot(x_axis, extended_t_loss, color=colors[i], linestyle='--', alpha=0.5,
                 label=f'Fold {i + 1} Train' if i < 3 else "")
        plt.plot(x_axis, extended_v_loss, color=colors[i], linewidth=2,
                 label=f'Fold {i + 1} Val' if i < 3 else "")

    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('MSE Loss', fontsize=12)
    plt.title(title, fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend(ncol=2, loc='upper right')
    plt.tight_layout()
    plt.show()
# 绘制训练曲线
plot_loss_curves(all_train_loss, all_val_loss, 'Training and Validation Loss Across Folds')

# 导出ONNX模型
final_model.eval()
dummy_input = torch.randn(1, X_final.shape[1])
torch.onnx.export(
    final_model,
    dummy_input,
    "boston_model.onnx",
    export_params=True,
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
    opset_version=11
)
在导出模型时，ONNX 需要知道输入的形状和数据类型，因此需要提供一个虚拟输入
export_params=True：表示将模型的参数一起导出到 ONNX 文件中。如果设置为 False，则只导出模型的结构，不包含参数

